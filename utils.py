import torch
from skimage.metrics import peak_signal_noise_ratio, structural_similarity
import logging
from tqdm import tqdm
import numpy as np


def psnr_slice(gt, pred, maxval=None):
    assert type(gt) == type(pred)
    if type(pred) is torch.Tensor:
        gt, pred = gt.detach().cpu().numpy(), pred.detach().cpu().numpy()
    batch_size = gt.shape[0]
    PSNR = 0.0
    for i in range(batch_size):
        max_val = gt[i].max() if maxval is None else maxval
        PSNR += peak_signal_noise_ratio(gt[i].squeeze(), pred[i].squeeze(), data_range=max_val)
    return PSNR / batch_size


def ssim_slice(gt, pred, maxval=None):
    assert type(gt) == type(pred)
    if type(pred) is torch.Tensor:
        gt, pred = gt.detach().cpu().numpy(), pred.detach().cpu().numpy()
    batch_size = gt.shape[0]
    SSIM = 0.0
    for i in range(batch_size):
        max_val = gt[i].max() if maxval is None else maxval
        SSIM += structural_similarity(gt[i].squeeze(), pred[i].squeeze(), data_range=max_val)
    return SSIM / batch_size


def center_crop(data, shape):
    assert 0 < shape[0] <= data.shape[-2]
    assert 0 < shape[1] <= data.shape[-1]
    w_from = (data.shape[-2] - shape[0]) // 2
    h_from = (data.shape[-1] - shape[1]) // 2
    w_to = w_from + shape[0]
    h_to = h_from + shape[1]
    return data[..., w_from:w_to, h_from:h_to]


def normalize_zero_to_one(data, eps=0.):
    data_min = float(data.min())
    data_max = float(data.max())
    return (data - data_min) / (data_max - data_min + eps)

import numpy as np
from numpy.lib.stride_tricks import as_strided


def normal_pdf(length, sensitivity):
    return np.exp(-sensitivity * (np.arange(length) - length / 2)**2)


def cartesian_mask(shape, acc, sample_n=None):
    """
    Sampling density estimated from implementation of kt FOCUSS
    shape: tuple - of form (..., nx, ny)
    acc: float - doesn't have to be integer 4, 8, etc..
    sample_n: preserve how many center lines (sample_n // 2 each side)
    """
    N, Nx, Ny = int(np.prod(shape[:-2])), shape[-2], shape[-1]
    pdf_x = normal_pdf(Nx, 0.5/(Nx/10.)**2)
    lmda = Nx/(2.*acc)
    n_lines = int(Nx / acc)

    # add uniform distribution
    pdf_x += lmda * 1./Nx

    if sample_n:
        pdf_x[Nx//2-sample_n//2:Nx//2+sample_n//2] = 0
        pdf_x /= np.sum(pdf_x)
        n_lines -= sample_n

    mask = np.zeros((N, Nx))
    for i in range(N):
        idx = np.random.choice(Nx, n_lines, False, pdf_x)
        mask[i, idx] = 1

    if sample_n:
        mask[:, Nx//2-sample_n//2:Nx//2+sample_n//2] = 1

    size = mask.itemsize
    mask = as_strided(mask, (N, Nx, Ny), (size * Nx, size, 0))

    mask = mask.reshape(shape)

    return mask



def create_logger(args):
    logger = logging.getLogger()
    logger.setLevel(level=logging.DEBUG)
    file_formatter = logging.Formatter('%(asctime)s %(filename)s [line:%(lineno)d] %(levelname)s:\t%(message)s')
    stream_formatter = logging.Formatter('%(levelname)s:\t%(message)s')

    file_handler = logging.FileHandler(filename=args.log_path, mode='a+', encoding='utf-8')
    file_handler.setLevel(level=logging.DEBUG)
    file_handler.setFormatter(file_formatter)

    stream_handler = logging.StreamHandler()
    stream_handler.setLevel(level=logging.INFO)
    stream_handler.setFormatter(stream_formatter)

    logger.addHandler(file_handler)
    logger.addHandler(stream_handler)
    return logger


class EarlyStopping:
    def __init__(self, patience=50, delta=0.0):
        self.patience = patience
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.delta = delta

    def __call__(self, metrics, loss=True):
        score = -metrics if loss else metrics
        if self.best_score is None:
            self.best_score = score
        elif score < self.best_score + self.delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.counter = 0


class SaveBest:
    def __init__(self, model):
        self.model = model
        self.best_score = model.best_score

    def __call__(self, score, higher=True):
        """higher is better"""
        better = score > self.best_score if higher else score < self.best_score
        if better:
            self.model.best_score = score
            self.model.save()
            self.model.savebest()


class Prefetch(torch.utils.data.Dataset):
    def __init__(self, dataset):
        super().__init__()
        self.dataset = [i for i in tqdm(dataset, leave=False)]

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, ind):
        return self.dataset[ind]



class AverageMeter(object):
    """
    Computes and stores the average and current value
    Copied from: https://github.com/pytorch/examples/blob/master/imagenet/main.py
    """
    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count


def dict2line(log: dict):
    to_join = list()
    for k in log:
        if 'epoch' in k:
            to_join.append(f'{k}:{log[k]}')
        if 'time' in k:
            to_join.append(f'{k}:{log[k]:.2f}s')
        if 'lr' in k:
            to_join.append(f'{k}:{log[k]:.1e}')
        if 'loss' in k:
            to_join.append(f'{k}:{log[k]:.2e}')
        if 'psnr' in k:
            to_join.append(f'{k}:{log[k]:.3f}')
        if 'ssim' in k:
            to_join.append(f'{k}:{log[k]:.5f}')

    line = ' | '.join(to_join)

    return line


def get_dataset(args):
    volumes_train = get_paired_volume_datasets(
            args.train, crop=256, protocals=['T2'],
            object_limit=args.train_obj_limit,
            u_mask_path=args.u_mask_path,
            s_mask_up_path=args.s_mask_up_path,
            s_mask_down_path=args.s_mask_down_path)
    volumes_val = get_paired_volume_datasets(
            args.val, crop=256, protocals=['T2'],
            object_limit=args.val_obj_limit,
            u_mask_path=args.u_mask_path,
            s_mask_up_path=args.s_mask_up_path,
            s_mask_down_path=args.s_mask_down_path
    )
    volumes_test = get_paired_volume_datasets(
            args.test, crop=256, protocals=['T2'],
            object_limit=args.test_obj_limit,
            u_mask_path=args.u_mask_path,
            s_mask_up_path=args.s_mask_up_path,
            s_mask_down_path=args.s_mask_down_path
    )
    slices_train = torch.utils.data.ConcatDataset(volumes_train)
    slices_val = torch.utils.data.ConcatDataset(volumes_val)
    slices_test = torch.utils.data.ConcatDataset(volumes_test)
    if args.prefetch:
        # load all data to ram
        slices_train = Prefetch(slices_train)
        slices_val = Prefetch(slices_val)
        slices_test = Prefetch(slices_test)

    return slices_train, slices_val, slices_test


def get_dataset_split(args, split):
    volumes = get_paired_volume_datasets(
            getattr(args, split), crop=256, protocals=['T2'],
            object_limit=getattr(args, split+'_obj_limit'),
            u_mask_path=args.u_mask_path,
            s_mask_up_path=args.s_mask_up_path,
            s_mask_down_path=args.s_mask_down_path)
    slices = torch.utils.data.ConcatDataset(volumes)
    if args.prefetch:
        # load all data to ram
        slices = Prefetch(slices)

    return slices


class SemisupervisedConcatDataset(torch.utils.data.ConcatDataset):
    def __init__(self, supervised_every, datasets):
        """
        supervised_every: one supervised volume every how many unsupervised volumes
            like supervised_every == 3, then 1 sup / 3 unsup
        """
        super(SemisupervisedConcatDataset, self).__init__(datasets)

        supervised_idx = list()
        supervised_volume_idx = list()
        unsupervised_idx = list()
        unsupervised_volume_idx = list()
        cur_start_idx = 0
        for i, dataset in enumerate(datasets):
            cur_end_idx = cur_start_idx + len(dataset)
            if (i+1) % supervised_every == 0:
                supervised_idx += list(range(cur_start_idx, cur_end_idx))
                supervised_volume_idx.append(i)
            else:
                unsupervised_idx += list(range(cur_start_idx, cur_end_idx))
                unsupervised_volume_idx.append(i)
            cur_start_idx += len(dataset)

        self.supervised_idx = supervised_idx
        self.unsupervised_idx = unsupervised_idx
        self.supervised_volume_idx = supervised_volume_idx
        self.unsupervised_volume_idx = unsupervised_volume_idx

        # print('sup volume idx', supervised_volume_idx)
        # print('unsup volume idx', unsupervised_volume_idx)
        # print('sup slice idx', self.supervised_idx)
        # print('unsup slice idx', self.unsupervised_idx)

        assert len(np.intersect1d(self.supervised_idx, self.unsupervised_idx)) == 0
        assert np.array_equal(np.sort(np.union1d(self.supervised_idx, self.unsupervised_idx)), np.arange(0, len(self)))


    def get_supervised_idxs(self):
        return self.supervised_idx

    def get_unsupervised_idxs(self):
        return self.unsupervised_idx


class SemisupervisedConcatDatasetV2(torch.utils.data.ConcatDataset):
    def __init__(self, unsupervised_datasets, supervised_datasets):
        super(SemisupervisedConcatDatasetV2, self).__init__(
            unsupervised_datasets +
            supervised_datasets
        )

        nuv = len(unsupervised_datasets)  # num of unsupervised volumes
        nus = np.sum([len(dataset) for dataset in unsupervised_datasets])  # num of unsupervised slices
        nsv = len(supervised_datasets)  # num of supervised volumes
        nss = np.sum([len(dataset) for dataset in supervised_datasets])  # num of supervised slices

        unsupervised_volume_idx = list(np.arange(nuv))
        unsupervised_slice_idx = list(np.arange(nus))
        supervised_volume_idx = list(np.arange(nuv, nuv + nsv))
        supervised_slice_idx = list(np.arange(nus, nus + nss))

        self.supervised_idx = supervised_slice_idx
        self.unsupervised_idx = unsupervised_slice_idx
        self.supervised_volume_idx = supervised_volume_idx
        self.unsupervised_volume_idx = unsupervised_volume_idx

        # print('unsup volume idx', unsupervised_volume_idx)
        # print('sup volume idx', supervised_volume_idx)
        # print('unsup slice idx', self.unsupervised_idx)
        # print('sup slice idx', self.supervised_idx)

        assert len(np.intersect1d(self.supervised_idx, self.unsupervised_idx)) == 0
        assert np.array_equal(np.sort(np.union1d(self.supervised_idx, self.unsupervised_idx)), np.arange(0, len(self)))

    def get_supervised_idxs(self):
        return self.supervised_idx

    def get_unsupervised_idxs(self):
        return self.unsupervised_idx


def get_semisupervised_dataset_split(args, split):
    volumes, unsupervised_volumes, supervised_volumes = get_paired_volume_datasets(
        getattr(args, split), crop=256, protocals=['T2'],
        object_limit=getattr(args, split+'_obj_limit'),
        u_mask_path=args.u_mask_path,
        s_mask_up_path=args.s_mask_up_path,
        s_mask_down_path=args.s_mask_down_path,
        supervised_every=args.supervised_every,
        semi_split=True
    )

    print(f'Total subjects: {len(volumes)}')
    print(f'Unsup subjects: {len(unsupervised_volumes)}')
    print(f'Sup subjects: {len(supervised_volumes)}')

    semi_slices = SemisupervisedConcatDatasetV2(unsupervised_volumes, supervised_volumes)
    unsup_slices = torch.utils.data.ConcatDataset(unsupervised_volumes) if len(unsupervised_volumes) > 0 else None
    sup_slices = torch.utils.data.ConcatDataset(supervised_volumes) if len(supervised_volumes) > 0 else None

    return semi_slices, unsup_slices, sup_slices







